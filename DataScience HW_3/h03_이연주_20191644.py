# -*- coding: utf-8 -*-
"""h03_이연주_20191644.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SL5u0BQQssJyrh2rUvaalkV_xOfyPtJ3
"""

#학습 데이터 생성

import torch

x_T = torch.FloatTensor([[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]])
y_T = torch.FloatTensor([[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]])

#W, b 초기화
#Optimizer 생성

W = torch.zeros(4, 3, requires_grad=True)
b = torch.zeros(1, 3, requires_grad=True)

optimizer = torch.optim.Adam([W, b], lr=0.1)

#반복횟수, 가설 및 비용 설정
#H(x) = S(x^T*w + b)

for epoch in range(3001):
  hypothesis = torch.softmax(torch.mm(x_T, W) + b, dim =1)
  cost = -torch.mean(torch.sum(y_T * torch.log(hypothesis), dim = 1))

  #Optimizer를 이용한 경사 계산 및 W, b 업데이트

  optimizer.zero_grad()
  cost.backward()
  optimizer.step()

  #Test

  if epoch % 300 == 0:
    print("Epoch : {}, cost : {:.6f}".format(epoch, cost.item()))

print("H : {}, Cost : {}".format(hypothesis, cost))

#x = [1, 11, 10, 9], [1, 3, 4, 3], [1, 1, 0, 1] ~> y =??

W.requires_grad_(False)
b.requires_grad_(False)

x_T = torch.FloatTensor([[1, 11, 10, 9], [1, 3, 4, 3], [1, 1, 0, 1]])
test_all = torch.softmax(torch.mm(x_T, W) + b, dim = 1)
print(test_all)
print(torch.argmax(test_all, dim = 1))

#조금 더 깔끔하게 softMax

import torch.nn.functional as F
import torch.nn as nn

x_T = torch.FloatTensor([[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]])
y_T = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0]) # == [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]

#W, b 초기화
#Optimizer 생성

W = torch.zeros(4, 3, requires_grad=True)
b = torch.zeros(1, 3, requires_grad=True)


model = nn.Linear(4, 3)
optimizer = torch.optim.Adam(model.parameters(), lr = 1) #optimizer = torch.optim.Adam([W, b], lr=0.1)

#반복횟수, 가설 및 비용 설정
#H(x) = S(x^T*w + b)

for epoch in range(3001):
  #z = torch.mm(x_T, W) + b
  z = model(x_T)
  cost = F.cross_entropy(z, y_T) # == hypothesis = toch.softmax(torch.mm(x_T, W) + b, dim = 1) 와 cost = -torch.mean(torch.sum(y_T * torch.log(hypothesis), dim = 1))

  #Optimizer를 이용한 경사 계산 및 W, b 업데이트

  optimizer.zero_grad()
  cost.backward()
  optimizer.step()

  #Test

  if epoch % 300 == 0:
    print("Epoch : {}, cost : {:.6f}".format(epoch, cost.item()))

print("H : {}, Cost : {}".format(hypothesis, cost.item())) 

'''
Epoch : 0, cost : 0.002584
Epoch : 300, cost : 0.002093
Epoch : 600, cost : 0.001713
Epoch : 900, cost : 0.001413
Epoch : 1200, cost : 0.001174
Epoch : 1500, cost : 0.000980
Epoch : 1800, cost : 0.000822
Epoch : 2100, cost : 0.000693
Epoch : 2400, cost : 0.000585
Epoch : 2700, cost : 0.000496
Epoch : 3000, cost : 0.000421
H : tensor([[1.2383e-32, 1.2456e-11, 1.0000e+00],
        [9.8307e-23, 1.1926e-04, 9.9988e-01],
        [1.3070e-41, 4.2518e-04, 9.9957e-01],
        [5.3429e-36, 9.9965e-01, 3.4725e-04],
        [8.5343e-04, 9.9912e-01, 2.6426e-05],
        [4.5186e-04, 9.9955e-01, 1.1364e-06],
        [9.9886e-01, 1.1428e-03, 1.8653e-12],
        [1.0000e+00, 2.5428e-08, 5.1274e-21]], grad_fn=<SoftmaxBackward>), Cost : 0.0004210831830278039
'''